# -*- coding: utf-8 -*-
"""
GUI: Novel -> Plot Graph DB (plot_nodes / plot_edges / plot_state / facts)
- é€‰æ‹©å°è¯´TXTï¼ŒæŒ‰ç« ä¸å°å—åˆ‡åˆ†
- ç”ŸæˆèŠ‚ç‚¹æ‘˜è¦/æç¤º/å…³é”®è¯ï¼ˆæ”¯æŒï¼šOllamaæœ¬åœ°å…è´¹ / OpenAI / çº¯è§„åˆ™ï¼‰
- facts ä¸‰å…ƒç»„â€œåªä½¿ç”¨ AI æŠ½å–â€ï¼ˆOllama æˆ– OpenAIï¼‰ï¼Œä¸å›é€€è§„åˆ™
- æ›´é²æ£’çš„ JSON è§£æã€é”®åå½’ä¸€åŒ–ä¸é˜²å¾¡å¼å…¥åº“ï¼Œé¿å… KeyErrorï¼ˆå¦‚ '"subject"'ï¼‰
- å†™å…¥SQLiteæ•°æ®åº“ï¼ˆå››å¼ è¡¨ï¼šplot_nodes / plot_edges / plot_state / factsï¼‰
- æ¯æœ¬ä¹¦ä¸€ä¸ªç‹¬ç«‹æ•°æ®åº“ï¼šé»˜è®¤ ./db/<ä¹¦å>.sqlite
- æ”¯æŒæ–‡æœ¬ç¼–ç ï¼šauto / utf-8 / gbk / cp936 / big5 / utf-8-sig
"""

import os
import re
import json
import sqlite3
import threading
import requests
from pathlib import Path
from typing import List, Dict, Any

# -------------------------- æ–‡æœ¬åˆ‡åˆ†é€»è¾‘ --------------------------
CHAPTER_PATTERNS = [
    r"^\s*ç¬¬[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹åç™¾åƒ0-9]{1,6}ç« [ï¼š:\s].*$",
    r"^\s*ç¬¬[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹åç™¾åƒ0-9]{1,6}èŠ‚[ï¼š:\s].*$",
    r"^\s*Chapter\s+\d+.*$",
    r"^\s*CHAPTER\s+\d+.*$",
]

def split_chapters(text: str):
    text = re.sub(
        r'(?<!\n)(ç¬¬[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹åç™¾åƒä¸‡0-9]{1,6}(?:[ç« èŠ‚å›é›†]|å·[ ã€€\t]*ç¬¬?[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹åç™¾åƒä¸‡0-9]{1,4}ç« )[^ã€‚\n\r]{0,60})',
        r'\n\1\n',
        text
    )
    patterns = [
        r'ç¬¬[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹åç™¾åƒä¸‡0-9]{1,6}\s*å·[ ã€€\t]*ç¬¬?[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹åç™¾åƒä¸‡0-9]{1,6}\s*ç« (?:[ï¼š:\-ã€,.ï¼Œã€‚]?[^\n\r]{0,60})?',
        r'ç¬¬[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹åç™¾åƒä¸‡0-9]{1,6}\s*[ç« èŠ‚å›é›†](?:[ï¼š:\-ã€,.ï¼Œã€‚]?[^\n\r]{0,60})?',
        r'[Cc][Hh][Aa][Pp][Tt][Ee][Rr]\s+\d+[^\n\r]{0,60}',
    ]
    chap_re = re.compile('|'.join(f'({p})' for p in patterns))
    matches = [(m.start(), m.group(0)) for m in chap_re.finditer(text)]
    if not matches:
        return [("ch001", "ç¬¬1ç« ", text)]
    matches = sorted(set(matches), key=lambda x: x[0])

    chapters = []
    for i, (start, title) in enumerate(matches):
        end = matches[i + 1][0] if i + 1 < len(matches) else len(text)
        body = text[start + len(title):end].strip()
        clean_title = title.strip().replace('\u3000', ' ')
        cid = f"ch{i+1:03d}"
        if len(clean_title) > 80:
            clean_title = clean_title[:80] + "â€¦"
        chapters.append((cid, clean_title, body))
    return chapters

def para_split(s: str) -> List[str]:
    parts = re.split(r"\n\s*\n+", s.strip())
    out = []
    for p in parts:
        p = p.strip()
        if not p:
            continue
        if len(p) <= 400:
            out.append(p)
        else:
            segs = re.split(r"(?<=[ã€‚ï¼ï¼Ÿ!?])", p)
            buf = ""
            for seg in segs:
                if len(buf) + len(seg) <= 500:
                    buf += seg
                else:
                    if buf:
                        out.append(buf.strip())
                    buf = seg
            if buf.strip():
                out.append(buf.strip())
    return out

def chunkify(text: str, max_chars: int = 700) -> List[str]:
    paras = para_split(text)
    chunks = []
    cur = ""
    for p in paras:
        if not cur:
            cur = p
            continue
        if len(cur) + 1 + len(p) <= max_chars:
            cur = cur + "\n" + p
        else:
            chunks.append(cur.strip())
            cur = p
    if cur.strip():
        chunks.append(cur.strip())
    final = []
    for c in chunks:
        if len(c) <= max_chars:
            final.append(c)
        else:
            for i in range(0, len(c), max_chars):
                final.append(c[i:i+max_chars].strip())
    return final

# -------------------------- ç¼–ç è‡ªåŠ¨/æ‰‹åŠ¨è¯»å– --------------------------
def read_text_with_encoding(novel_path: str, preferred: str = "auto") -> str:
    p = Path(novel_path)
    if preferred and preferred.lower() != "auto":
        return p.read_text(encoding=preferred, errors="strict")
    try:
        from charset_normalizer import from_path
        res = from_path(str(p)).best()
        if res and res.encoding:
            return p.read_text(encoding=res.encoding, errors="strict")
    except Exception:
        pass
    for enc in ("utf-8-sig", "utf-8", "gbk", "cp936", "big5"):
        try:
            return p.read_text(encoding=enc, errors="strict")
        except Exception:
            continue
    return p.read_text(encoding="utf-8", errors="ignore")

# -------------------------- ç”Ÿæˆæ‘˜è¦/æç¤ºï¼ˆè§„åˆ™å…œåº•ï¼‰ --------------------------
def rule_fallback_enrich(title: str, content: str) -> Dict[str, str]:
    summary = (content.replace("\n","")[:58] + "â€¦") if len(content) > 60 else content
    entry_hint = (f"è¿›å…¥ï¼š{title}"[:24]) if title else "è¿›å…¥å½“å‰ç‰‡æ®µ"
    exit_hint = "ç»§ç»­æ¨è¿›å‰§æƒ…"
    words = re.findall(r"[\u4e00-\u9fa5A-Za-z0-9]{2,}", content)
    uniq = []
    for w in words:
        if len(w) >= 2 and w not in uniq:
            uniq.append(w)
        if len(uniq) >= 10:
            break
    fact_tags = ",".join(uniq)
    return dict(summary=summary, entry_hint=entry_hint, exit_hint=exit_hint, fact_tags=fact_tags)

# -------------------------- Ollama æœ¬åœ°è°ƒç”¨ --------------------------
def ollama_chat(model: str, sys_prompt: str, user_prompt: str, host: str = "http://127.0.0.1:11434",
                use_json: bool = False, options: dict = None) -> str:
    """è°ƒç”¨ Ollama /api/chatï¼›æ”¯æŒ format=jsonï¼ˆè‹¥æ¨¡å‹å…¼å®¹ï¼‰ä¸é‡‡æ ·å‚æ•°ã€‚"""
    url = f"{host}/api/chat"
    payload = {
        "model": model,
        "messages": [
            {"role": "system", "content": sys_prompt.strip()},
            {"role": "user",  "content": user_prompt.strip()},
        ],
        "stream": False,
    }
    if use_json:
        # è®¸å¤š Instruct æ¨¡å‹å·²æ”¯æŒç»“æ„åŒ– JSON è¾“å‡º
        payload["format"] = "json"
    if options:
        payload["options"] = options

    resp = requests.post(url, json=payload, timeout=120)
    resp.raise_for_status()
    data = resp.json()
    return data["message"]["content"]


def ollama_chat_safe(model: str, sys_prompt: str, user_prompt: str, host: str = "http://127.0.0.1:11434") -> str:
    """ä¼˜å…ˆå°è¯• JSON æ ¼å¼ï¼›å¤±è´¥åˆ™æ™®é€šæ¨¡å¼å›é€€ã€‚"""
    opts = {"temperature": 0.0, "top_p": 0.9, "repeat_penalty": 1.1, "num_ctx": 4096}
    try:
        return ollama_chat(model, sys_prompt, user_prompt, host=host, use_json=True, options=opts)
    except Exception:
        return ollama_chat(model, sys_prompt, user_prompt, host=host, use_json=False, options=opts)


def ai_enrich_ollama(model: str, title: str, content: str) -> Dict[str, str]:
    sys_prompt = "ä½ æ˜¯å°è¯´ç»“æ„åŒ–åŠ©æ‰‹ï¼Œè¿”å›ä¸¥æ ¼ JSONã€‚"
    user_prompt = f"""
è¯·åŸºäºä¸‹é¢â€œæ ‡é¢˜+æ­£æ–‡ç‰‡æ®µâ€ç”Ÿæˆ4ä¸ªå­—æ®µï¼š
1) summaryï¼šä¸è¶…è¿‡60å­—çš„ä¸­æ–‡æ‘˜è¦ï¼Œå®¢è§‚é™ˆè¿°ã€‚
2) entry_hintï¼šè¿™ä¸€æ®µå¼€å§‹å‰çš„å¼•å¯¼è¯­ï¼Œä¸è¶…è¿‡25å­—ã€‚
3) exit_hintï¼šè¯»å®Œè¿™ä¸€æ®µåçš„è¿‡æ¸¡æç¤ºï¼Œä¸è¶…è¿‡25å­—ã€‚
4) fact_tagsï¼šé€—å·åˆ†éš”çš„å…³é”®è¯ï¼ˆäººç‰©ã€åœ°ç‚¹ã€ç‰©ä»¶ã€äº‹ä»¶ï¼‰ï¼Œæœ€å¤š12ä¸ªã€‚

ã€æ ‡é¢˜ã€‘{title}
ã€æ­£æ–‡ç‰‡æ®µã€‘{content[:1200]}

åªè¿”å› JSON ä¸”ä»…åŒ…å«è¿™4ä¸ªé”®ï¼šsummary, entry_hint, exit_hint, fact_tags
"""
    try:
        txt = ollama_chat(model, sys_prompt, user_prompt)
        try:
            data = json.loads(txt)
        except Exception:
            data = {}
            for k in ("summary","entry_hint","exit_hint","fact_tags"):
                m = re.search(rf'"?{k}"?\s*[:ï¼š]\s*"(.*?)"', txt)
                if m:
                    data[k] = m.group(1)
        base = rule_fallback_enrich(title, content)
        for k, v in base.items():
            data.setdefault(k, v)
        return data
    except Exception:
        return rule_fallback_enrich(title, content)

# -------------------------- OpenAIï¼ˆå¯é€‰ï¼‰ --------------------------
def get_openai_client():
    api_key = os.getenv("OPENAI_API_KEY", "").strip()
    model = os.getenv("OPENAI_CHAT_MODEL", "gpt-4.1-mini").strip()
    if not api_key:
        return None, None
    try:
        from openai import OpenAI
        client = OpenAI(api_key=api_key)
        return client, model
    except Exception:
        return None, None

def ai_enrich_openai(client, model: str, title: str, content: str) -> Dict[str, str]:
    if client is None:
        return rule_fallback_enrich(title, content)
    prompt = f"""
ä½ æ˜¯æ•´ç†é•¿ç¯‡å°è¯´ç»“æ„çš„åŠ©æ‰‹ã€‚è¯·åŸºäºä¸‹é¢â€œæ ‡é¢˜+æ­£æ–‡ç‰‡æ®µâ€ç”Ÿæˆ4ä¸ªå­—æ®µï¼š
1) summaryï¼šä¸è¶…è¿‡60å­—çš„ä¸­æ–‡æ‘˜è¦ï¼Œå®¢è§‚é™ˆè¿°ã€‚
2) entry_hintï¼šè¿™ä¸€æ®µå¼€å§‹å‰çš„å¼•å¯¼è¯­ï¼Œä¸è¶…è¿‡25å­—ã€‚
3) exit_hintï¼šè¯»å®Œè¿™ä¸€æ®µåçš„è¿‡æ¸¡æç¤ºï¼Œä¸è¶…è¿‡25å­—ã€‚
4) fact_tagsï¼šé€—å·åˆ†éš”çš„å…³é”®è¯ï¼ˆäººç‰©ã€åœ°ç‚¹ã€ç‰©ä»¶ã€äº‹ä»¶ï¼‰ï¼Œä¸è¶…è¿‡12ä¸ªã€‚

ã€æ ‡é¢˜ã€‘{title}
ã€æ­£æ–‡ç‰‡æ®µã€‘{content[:1200]}
åªè¿”å›JSONï¼Œé”®åŒ…å«ï¼šsummary, entry_hint, exit_hint, fact_tagsã€‚
"""
    try:
        resp = client.chat.completions.create(
            model=model,
            messages=[{"role":"user","content":prompt}],
            temperature=0.2,
        )
        txt = resp.choices[0].message.content.strip()
        try:
            data = json.loads(txt)
        except Exception:
            data = {}
            for k in ("summary","entry_hint","exit_hint","fact_tags"):
                m = re.search(rf'"?{k}"?\s*[:ï¼š]\s*"(.*?)"', txt)
                if m:
                    data[k] = m.group(1)
        base = rule_fallback_enrich(title, content)
        for k, v in base.items():
            data.setdefault(k, v)
        return data
    except Exception:
        return rule_fallback_enrich(title, content)

# -------------------------- ç»Ÿä¸€å…¥å£ï¼šèŠ‚ç‚¹å¯ŒåŒ– --------------------------
def ai_enrich_unified(provider: str,
                      title: str,
                      content: str,
                      openai_client=None,
                      openai_model=None,
                      ollama_model=None):
    provider = (provider or "ollama").lower()
    if provider == "ollama":
        return ai_enrich_ollama(ollama_model or "llama3.1:8b-instruct", title, content)
    elif provider == "openai":
        return ai_enrich_openai(openai_client, openai_model or "gpt-4.1-mini", title, content)
    else:
        return rule_fallback_enrich(title, content)

# -------------------------- facts æŠ½å–ï¼ˆAI-onlyï¼›é²æ£’è§£æ + é”®åå½’ä¸€ + é˜²å¾¡å…¥åº“ï¼‰ --------------------------
def _json_loads_relaxed(s: str):
    if not s:
        return None
    s = s.strip()
    if s.startswith("```"):
        s = re.sub(r"^```[a-zA-Z0-9]*\s*", "", s)
        s = re.sub(r"\s*```$", "", s)
    try:
        return json.loads(s)
    except Exception:
        pass
    m = re.search(r'(\{(?:[^{}]|\{[^{}]*\})*\}|\[(?:[^\[\]]|\[[^\[\]]*\])*\])', s, re.S)
    if m:
        frag = m.group(1)
        try:
            return json.loads(frag)
        except Exception:
            return None
    return None

# é”®åæ˜ å°„ä¸å½’ä¸€åŒ–ï¼Œå…¼å®¹ä¸­æ–‡/åˆ«å
_CANON_MAP = {
    "subject": ["subject", "ä¸»è¯­", "å®ä½“", "S", "s", "subj", "entity"],
    "predicate": ["predicate", "è°“è¯", "å…³ç³»", "P", "p", "pred", "relation"],
    "object": ["object", "å®¾è¯­", "O", "o", "obj", "å¯¹è±¡", "ç›®æ ‡"],
    "summary": ["summary", "æ‘˜è¦", "è¯´æ˜", "æ³¨é‡Š", "ç®€è¿°", "æ¦‚è¿°"],
    "evidence": ["evidence", "è¯æ®", "å‡ºå¤„", "evidences", "spans"],
}

def _get_first(it: Dict[str, Any], keys: List[str]) -> str:
    for k in keys:
        v = it.get(k)
        if v is None:
            continue
        if isinstance(v, (str, int, float)):
            return str(v).strip()
        # æŸäº›æ¨¡å‹æŠŠå€¼åŒ…æˆ {"text":"..."} æˆ– ["..."]
        if isinstance(v, dict):
            if "text" in v and isinstance(v["text"], str):
                return v["text"].strip()
        if isinstance(v, list) and v and isinstance(v[0], str):
            return v[0].strip()
    return ""

def _normalize_facts_payload(data, chunk_id: str) -> List[Dict[str, str]]:
    # å…è®¸ï¼š1) {"facts":[...]} 2) ç›´æ¥ [...] 3) {"triples"/"data"/"result"/"items":[...]}
    if isinstance(data, list):
        facts_list = data
    elif isinstance(data, dict):
        facts_list = None
        for k in ("facts", "triples", "data", "result", "items"):
            v = data.get(k)
            if isinstance(v, list):
                facts_list = v
                break
        if facts_list is None:
            return []
    else:
        return []

    out = []
    for it in facts_list:
        if not isinstance(it, dict):
            continue
        s = _get_first(it, _CANON_MAP["subject"])
        p = _get_first(it, _CANON_MAP["predicate"])
        o = _get_first(it, _CANON_MAP["object"])
        sm = _get_first(it, _CANON_MAP["summary"])
        ev_raw = it.get("evidence") or it.get("è¯æ®") or it.get("evidences") or it.get("spans") or []
        ev: List[str] = []
        if isinstance(ev_raw, list):
            ev = [str(x) for x in ev_raw if isinstance(x, (str, int, float))]
        if not ev:
            ev = [f"{{{{{chunk_id}}}}}"] if chunk_id else []

        if s and p and o:
            out.append({
                "subject": s, "predicate": p, "object": o,
                "summary": sm, "evidence": ev
            })
    return out[:12]

_AI_SYS_FACTS = "ä½ æ˜¯æŠ½å–å™¨ã€‚åªè¿”å› JSONï¼Œä¸è¦å¤šä½™æ–‡å­—ã€‚ç¼ºä¿¡æ¯ç•™ç©ºã€‚"
_AI_USER_FACTS_TPL = """è¾“å…¥æ˜¯ä¸€æ®µå°è¯´æ–‡æœ¬ç‰‡æ®µï¼Œè¯·æŠ½å–â€œå¯éªŒè¯è®¾å®š/è§„åˆ™ï¼ˆfactsï¼‰â€ï¼Œå¹¶ç”ŸæˆçŸ­æ‘˜è¦ summaryã€‚
ä¸¥æ ¼è¿”å›ä¸€ä¸ª JSON å¯¹è±¡ï¼Œé”®ä¸º factsï¼Œå€¼æ˜¯æ•°ç»„ã€‚æ•°ç»„å…ƒç´ å¿…é¡»ä¸ºï¼š
{"subject":"<å®ä½“åæˆ–è§„åˆ™åŸŸ>","predicate":"...","object":"...","summary":"<ç®€æ´å¯æ£€ç´¢>","evidence":["{{chunk_id}}"]}

ç¤ºä¾‹ï¼š
{"facts":[{"subject":"è§„åˆ™A","predicate":"æ¡ä»¶","object":"è§¦å‘","summary":"A æ»¡è¶³æ¡ä»¶å³è§¦å‘","evidence":["{{chunk_id}}"]}]}

ã€æ ‡é¢˜ã€‘{title}
ã€chunk_idã€‘{chunk_id}
ã€æ–‡æœ¬ã€‘{text}"""

def ai_extract_facts_ollama(model: str, title: str, content: str, chunk_id: str,
                            host: str = "http://127.0.0.1:11434") -> List[Dict[str,str]]:
    sys_prompt = _AI_SYS_FACTS
    user_prompt = _AI_USER_FACTS_TPL.format(
        title=title, chunk_id=chunk_id or "", text=content[:1500]
    )
    try:
        txt = ollama_chat(model, sys_prompt, user_prompt, host=host)
        data = _json_loads_relaxed(txt)
        return _normalize_facts_payload(data, chunk_id)
    except Exception:
        return []

def ai_extract_facts_openai(client, model: str, title: str, content: str, chunk_id: str) -> List[Dict[str,str]]:
    if client is None:
        return []
    sys_prompt = _AI_SYS_FACTS
    user_prompt = _AI_USER_FACTS_TPL.format(
        title=title, chunk_id=chunk_id or "", text=content[:1500]
    )
    try:
        resp = client.chat.completions.create(
            model=model,
            messages=[{"role":"system","content":sys_prompt},{"role":"user","content":user_prompt}],
            temperature=0.0,
        )
        txt = resp.choices[0].message.content.strip()
        data = _json_loads_relaxed(txt)
        return _normalize_facts_payload(data, chunk_id)
    except Exception:
        return []

def extract_facts_unified(provider: str,
                          title: str,
                          content: str,
                          openai_client=None,
                          openai_model: str = None,
                          ollama_model: str = None,
                          chunk_id: str = "") -> List[Dict[str,str]]:
    provider = (provider or "ollama").lower()
    if provider == "ollama":
        return ai_extract_facts_ollama(ollama_model or "qwen2.5:7b-instruct", title, content, chunk_id)
    elif provider == "openai":
        return ai_extract_facts_openai(openai_client, openai_model or "gpt-4.1-mini", title, content, chunk_id)
    else:
        return []

# -------------------------- æ•°æ®åº“ --------------------------
SCHEMA_SQL = """
             CREATE TABLE IF NOT EXISTS plot_nodes (
                                                       id             TEXT PRIMARY KEY,
                                                       title          TEXT NOT NULL,
                                                       summary        TEXT NOT NULL,
                                                       entry_hint     TEXT,
                                                       exit_hint      TEXT,
                                                       fact_tags      TEXT,
                                                       required_flags TEXT,
                                                       set_flags      TEXT
             );

             CREATE TABLE IF NOT EXISTS plot_edges (
                                                       id         INTEGER PRIMARY KEY AUTOINCREMENT,
                                                       src        TEXT NOT NULL,
                                                       dst        TEXT NOT NULL,
                                                       condition  TEXT,
                                                       keywords   TEXT,
                                                       UNIQUE(src, dst)
                 );

             CREATE TABLE IF NOT EXISTS plot_state (
                                                       save_id      TEXT PRIMARY KEY DEFAULT 'default',
                                                       current_node TEXT NOT NULL
             );

/* facts è¡¨ï¼ˆä¾› interactive ä½¿ç”¨ï¼‰ */
             CREATE TABLE IF NOT EXISTS facts (
                                                  id        INTEGER PRIMARY KEY AUTOINCREMENT,
                                                  subject   TEXT,
                                                  predicate TEXT,
                                                  object    TEXT,
                                                  summary   TEXT,
                                                  UNIQUE(subject, predicate, object)
                 );

             CREATE INDEX IF NOT EXISTS idx_facts_sp ON facts(subject, predicate);
             CREATE INDEX IF NOT EXISTS idx_facts_po ON facts(predicate, object); \
             """

def ensure_db(db_path: str):
    Path(db_path).parent.mkdir(parents=True, exist_ok=True)
    conn = sqlite3.connect(db_path)
    with conn:
        conn.executescript(SCHEMA_SQL)
    return conn

# -------------------------- ä¸»æµç¨‹ --------------------------
def build_graph_from_novel(novel_path: str,
                           db_path: str,
                           provider: str = "ollama",
                           ollama_model: str = "llama3.1:8b-instruct",
                           max_chunk_chars: int = 700,
                           overwrite: bool = False,
                           encoding: str = "auto",
                           make_facts: bool = True,
                           log=lambda msg: None):
    novel_path = Path(novel_path)
    if not novel_path.exists():
        raise FileNotFoundError(f"å°è¯´ä¸å­˜åœ¨ï¼š{novel_path}")

    db_path = Path(db_path)
    if db_path.exists() and overwrite:
        try:
            db_path.unlink()
        except Exception as e:
            raise RuntimeError(f"æ— æ³•åˆ é™¤æ—§æ•°æ®åº“ï¼š{db_path}ï¼ŒåŸå› ï¼š{e}")
    elif db_path.exists() and not overwrite:
        raise FileExistsError(f"æ•°æ®åº“å·²å­˜åœ¨ï¼ˆä¸ºé¿å…è¦†ç›–ï¼Œé»˜è®¤ä¸é‡å»ºï¼‰ï¼š{db_path}")

    text = read_text_with_encoding(str(novel_path), preferred=encoding)
    log(f"æ–‡æœ¬è¯»å–å®Œæˆï¼ˆencoding={encoding})")
    chapters = split_chapters(text)
    log(f"æ£€æµ‹åˆ°ç« èŠ‚æ•°ï¼š{len(chapters)}")

    openai_client, openai_model = (get_openai_client() if provider == "openai" else (None, None))
    conn = ensure_db(str(db_path))
    cur = conn.cursor()

    node_ids_in_order: List[str] = []
    total_nodes = 0
    facts_bucket = set()

    for cid, ctitle, cbody in chapters:
        chunks = chunkify(cbody, max_chars=max_chunk_chars)
        if not chunks:
            chunks = ["ï¼ˆç©ºç« èŠ‚å†…å®¹ï¼‰"]
        log(f"å¤„ç† {ctitle}ï¼šåˆ‡åˆ†å— {len(chunks)}")

        for i, chunk in enumerate(chunks, 1):
            node_id = f"{cid}_{i:03d}"
            node_title = f"{ctitle} Â· {i}"

            enrich = ai_enrich_unified(
                provider=provider,
                title=node_title,
                content=chunk,
                openai_client=openai_client,
                openai_model=openai_model,
                ollama_model=ollama_model,
            )

            required_flags = ""
            set_flags = node_id

            cur.execute(
                """INSERT OR REPLACE INTO plot_nodes
                   (id, title, summary, entry_hint, exit_hint, fact_tags, required_flags, set_flags)
                   VALUES (?, ?, ?, ?, ?, ?, ?, ?)""",
                (node_id, node_title, enrich["summary"], enrich["entry_hint"],
                 enrich["exit_hint"], enrich["fact_tags"], required_flags, set_flags)
            )

            # æŠ½å–å¹¶æ”¶é›† factsï¼ˆä»… AIï¼‰ï¼Œé˜²å¾¡å¼è¿‡æ»¤
            if make_facts:
                try:
                    facts = extract_facts_unified(
                        provider=provider,
                        title=node_title,
                        content=chunk,
                        openai_client=openai_client,
                        openai_model=openai_model,
                        ollama_model=ollama_model,
                        chunk_id=node_id,
                    )
                except Exception:
                    facts = []

                if facts:
                    for f in facts:
                        try:
                            if isinstance(f, dict):
                                s = (f.get("subject") or "").strip()
                                p = (f.get("predicate") or "").strip()
                                o = (f.get("object") or "").strip()
                                sm = (f.get("summary") or "").strip()
                            elif isinstance(f, (list, tuple)) and len(f) >= 3:
                                s, p, o = [str(x).strip() for x in f[:3]]
                                sm = str(f[3]).strip() if len(f) > 3 else ""
                            else:
                                continue
                            if s and p and o:
                                facts_bucket.add((s, p, o, sm))
                        except Exception:
                            continue

            node_ids_in_order.append(node_id)
            total_nodes += 1

    for a, b in zip(node_ids_in_order[:-1], node_ids_in_order[1:]):
        cur.execute(
            "INSERT OR IGNORE INTO plot_edges (src, dst, condition, keywords) VALUES (?, ?, ?, ?)",
            (a, b, "é¡ºåºæ¨è¿›", "next")
        )

    if node_ids_in_order:
        first_node = node_ids_in_order[0]
        cur.execute(
            "INSERT OR REPLACE INTO plot_state (save_id, current_node) VALUES (?, ?)",
            ("default", first_node)
        )

    if make_facts and facts_bucket:
        cur.executemany(
            "INSERT OR IGNORE INTO facts (subject, predicate, object, summary) VALUES (?, ?, ?, ?)",
            list(facts_bucket)
        )

    conn.commit()
    conn.close()

    log(f"âœ… å®Œæˆï¼š{len(chapters)} ç« ï¼Œå…± {total_nodes} ä¸ªèŠ‚ç‚¹ â†’ {db_path}")
    if make_facts:
        log(f"ğŸ“š å·²å†™å…¥ factsï¼š{len(facts_bucket)} æ¡ï¼ˆå»é‡åï¼‰")
    return total_nodes

# -------------------------- å®ç”¨å‡½æ•° --------------------------
def slug_from_input(path_str: str) -> str:
    name = Path(path_str).stem
    cleaned = "".join(ch for ch in name if ch not in "\r\n\t").strip().replace(" ", "_")
    return cleaned or "book"

# -------------------------- GUI --------------------------
import tkinter as tk
from tkinter import ttk, filedialog, messagebox

class App(tk.Tk):
    def __init__(self):
        super().__init__()
        self.title("Novel â†’ Plot DB ç”Ÿæˆå™¨ï¼ˆOllama/OpenAI/è§„åˆ™ + factsä»…AI + é”®åå½’ä¸€ï¼‰")
        self.geometry("900x680")

        self.var_input = tk.StringVar()
        self.var_provider = tk.StringVar(value="ollama")
        self.var_ollama_model = tk.StringVar(value="qwen2.5:7b-instruct")
        self.var_openai_model = tk.StringVar(value=os.getenv("OPENAI_CHAT_MODEL", "gpt-4.1-mini"))
        self.var_max_chars = tk.IntVar(value=700)
        self.var_db = tk.StringVar(value="")
        self.var_overwrite = tk.BooleanVar(value=False)
        self.var_encoding = tk.StringVar(value="auto")
        self.var_make_facts = tk.BooleanVar(value=True)

        self._build_ui()

    def _build_ui(self):
        pad = {"padx": 8, "pady": 6}
        frm = ttk.Frame(self); frm.pack(fill="x", **pad)

        ttk.Label(frm, text="å°è¯´TXTï¼š").grid(row=0, column=0, sticky="w")
        ttk.Entry(frm, textvariable=self.var_input, width=72).grid(row=0, column=1, sticky="we")
        ttk.Button(frm, text="æµè§ˆâ€¦", command=self._browse_input).grid(row=0, column=2, sticky="w")

        ttk.Label(frm, text="æ‘˜è¦æä¾›æ–¹ï¼š").grid(row=1, column=0, sticky="w")
        cmb = ttk.Combobox(frm, textvariable=self.var_provider, state="readonly",
                           values=["ollama","openai","rule"], width=12)
        cmb.grid(row=1, column=1, sticky="w")
        cmb.bind("<<ComboboxSelected>>", lambda e: self._render_model_fields())

        self.frm_model = ttk.Frame(frm); self.frm_model.grid(row=2, column=0, columnspan=3, sticky="we")
        self._render_model_fields()

        ttk.Label(frm, text="å°å—æœ€å¤§å­—æ•°ï¼š").grid(row=3, column=0, sticky="w")
        ttk.Spinbox(frm, from_=200, to=2000, textvariable=self.var_max_chars, width=10).grid(row=3, column=1, sticky="w")

        ttk.Label(frm, text="æ–‡æœ¬ç¼–ç ï¼š").grid(row=4, column=0, sticky="w")
        ttk.Combobox(frm, textvariable=self.var_encoding, state="readonly",
                     values=["auto","utf-8","utf-8-sig","gbk","cp936","big5"]).grid(row=4, column=1, sticky="w")

        ttk.Label(frm, text="æ•°æ®åº“æ–‡ä»¶ï¼ˆç•™ç©º=è‡ªåŠ¨ï¼‰ï¼š").grid(row=5, column=0, sticky="w")
        ttk.Entry(frm, textvariable=self.var_db, width=72).grid(row=5, column=1, sticky="we")
        ttk.Button(frm, text="é€‰æ‹©â€¦", command=self._browse_db).grid(row=5, column=2, sticky="w")

        ttk.Checkbutton(frm, text="å…è®¸è¦†ç›–å·²å­˜åœ¨çš„åŒåæ•°æ®åº“ï¼ˆä¼šåˆ é™¤æ—§åº“ï¼‰", variable=self.var_overwrite).grid(row=6, column=1, sticky="w")
        ttk.Checkbutton(frm, text="åŒæ—¶æŠ½å– factsï¼ˆä¸‰å…ƒç»„ï¼Œä»…AIæŠ½å–ï¼‰", variable=self.var_make_facts).grid(row=7, column=1, sticky="w")

        ttk.Button(self, text="å¼€å§‹ç”Ÿæˆ", command=self._run_thread).pack(**pad)

        self.txt = tk.Text(self, height=22); self.txt.pack(fill="both", expand=True, **pad)
        self._log("å‡†å¤‡å°±ç»ªã€‚é»˜è®¤ä½¿ç”¨ Ollama æœ¬åœ°æ¨¡å‹ï¼ˆqwen2.5:7b-instructï¼‰ã€‚facts æŠ½å–ä»…ä½¿ç”¨ AIï¼ˆå«é”®åå½’ä¸€ï¼‰ã€‚")

    def _render_model_fields(self):
        for w in self.frm_model.winfo_children():
            w.destroy()
        provider = self.var_provider.get()
        if provider == "ollama":
            ttk.Label(self.frm_model, text="Ollamaæ¨¡å‹ï¼š").grid(row=0, column=0, sticky="w")
            ttk.Entry(self.frm_model, textvariable=self.var_ollama_model, width=30).grid(row=0, column=1, sticky="w")
            ttk.Label(self.frm_model, text="ç¤ºä¾‹ï¼šqwen2.5:7b-instruct / llama3.1:8b-instruct").grid(row=0, column=2, sticky="w")
        elif provider == "openai":
            ttk.Label(self.frm_model, text="OpenAIæ¨¡å‹ï¼š").grid(row=0, column=0, sticky="w")
            ttk.Entry(self.frm_model, textvariable=self.var_openai_model, width=30).grid(row=0, column=1, sticky="w")
            ttk.Label(self.frm_model, text="ç¤ºä¾‹ï¼šgpt-4.1-miniï¼ˆéœ€è®¾ç½® OPENAI_API_KEYï¼‰").grid(row=0, column=2, sticky="w")
        else:
            ttk.Label(self.frm_model, text="å½“å‰ä½¿ç”¨çº¯è§„åˆ™æ¨¡å¼ï¼ˆä»…ç”¨äºæ‘˜è¦å¯ŒåŒ–ï¼‰ï¼›facts æŠ½å–è¯·åœ¨ä¸Šæ–¹é€‰æ‹© AI æä¾›æ–¹").grid(row=0, column=0, sticky="w")

    def _browse_input(self):
        path = filedialog.askopenfilename(title="é€‰æ‹©å°è¯´TXT", filetypes=[("Text","*.txt"),("All","*.*")])
        if path:
            self.var_input.set(path)
            slug = slug_from_input(path)
            auto_db = str(Path("./db") / f"{slug}.sqlite")
            auto_db = auto_db.replace("}}", "}")  # å®¹é”™
            if not self.var_db.get():
                self.var_db.set(auto_db)

    def _browse_db(self):
        path = filedialog.asksaveasfilename(title="é€‰æ‹©æˆ–è¾“å…¥æ•°æ®åº“æ–‡ä»¶å",
                                            initialfile=self.var_db.get() or "book.sqlite",
                                            defaultextension=".sqlite",
                                            filetypes=[("SQLite DB","*.sqlite;*.db"),("All","*.*")])
        if path:
            self.var_db.set(path)

    def _log(self, msg: str):
        self.txt.insert("end", msg + "\n")
        self.txt.see("end")
        self.update_idletasks()

    def _run_thread(self):
        t = threading.Thread(target=self._run, daemon=True)
        t.start()

    def _run(self):
        novel = self.var_input.get().strip()
        if not novel:
            messagebox.showwarning("æç¤º", "è¯·é€‰æ‹©å°è¯´TXTæ–‡ä»¶")
            return
        db = self.var_db.get().strip()
        if not db:
            slug = slug_from_input(novel)
            db = str(Path("./db") / f"{slug}.sqlite")
            self.var_db.set(db)

        provider = self.var_provider.get().strip().lower()
        max_chars = int(self.var_max_chars.get())
        overwrite = bool(self.var_overwrite.get())
        ollama_model = self.var_ollama_model.get().strip() or "qwen2.5:7b-instruct"
        encoding = self.var_encoding.get().strip().lower()
        make_facts = bool(self.var_make_facts.get())

        self._log(f"è¾“å…¥ï¼š{novel}")
        self._log(f"æ•°æ®åº“ï¼š{db}")
        self._log(f"æ¨¡å¼ï¼š{provider}ï¼Œæ¨¡å‹ï¼š{ollama_model if provider=='ollama' else self.var_openai_model.get()}")
        self._log(f"å°å—æœ€å¤§å­—æ•°ï¼š{max_chars}ï¼Œè¦†ç›–ï¼š{overwrite}ï¼Œç¼–ç ï¼š{encoding}")

        if provider == "ollama":
            try:
                r = requests.get("http://127.0.0.1:11434/api/tags", timeout=3)
                if r.status_code == 200:
                    self._log("Ollama æœåŠ¡å·²è¿æ¥ã€‚")
            except Exception:
                self._log("âš  æ— æ³•è¿æ¥ Ollama æœåŠ¡ï¼ˆè¯·å…ˆè¿è¡Œï¼šollama serveï¼‰ï¼Œå°†ç»§ç»­å°è¯•è°ƒç”¨ã€‚")
        elif provider == "openai":
            if not os.getenv("OPENAI_API_KEY"):
                self._log("âš  æœªæ£€æµ‹åˆ° OPENAI_API_KEYï¼ŒOpenAI è°ƒç”¨å°†å¤±è´¥ï¼›facts æŠ½å–ä¸ä¼šå†™å…¥ã€‚")

        try:
            total = build_graph_from_novel(
                novel_path=novel,
                db_path=db,
                provider=provider,
                ollama_model=ollama_model,
                max_chunk_chars=max_chars,
                overwrite=overwrite,
                encoding=encoding,
                make_facts=make_facts,
                log=self._log
            )
            self._log(f"ğŸ‰ å®Œæˆï¼å…±å†™å…¥ {total} ä¸ªèŠ‚ç‚¹ã€‚")
        except FileExistsError as e:
            self._log(f"âŒ {e}")
            messagebox.showerror("å·²å­˜åœ¨", str(e))
        except Exception as e:
            self._log(f"âŒ å¤±è´¥ï¼š{e}")
            messagebox.showerror("é”™è¯¯", str(e))

def main():
    app = App()
    app.mainloop()

if __name__ == "__main__":
    main()
