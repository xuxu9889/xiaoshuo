import re, os, chardet, time
from pathlib import Path
from llm_io import embed
from store import ensure_schema, get_sql, add_chunks

RAW = "data/raw/book.txt"

# è‡ªåŠ¨æ£€æµ‹ç¼–ç 
def read_file_auto_encoding(path):
    with open(path, 'rb') as f:
        data = f.read()
    enc = chardet.detect(data)['encoding'] or 'utf-8'
    print(f"Detected encoding: {enc}")
    return data.decode(enc, errors='ignore')

# æ¸…ç†æ–‡æœ¬
def clean_text(t: str) -> str:
    t = t.replace("\r","")
    t = re.sub(r"[ \t]+", " ", t)
    t = re.sub(r"\n{2,}", "\n", t)
    return t.strip()

# æŒ‰å¥å·/æ¢è¡Œåˆ‡ç‰‡
def chunk_text(text: str, max_len=900, overlap=80):
    parts = re.split(r"(ã€‚|ï¼|ï¼Ÿ|\n)", text)
    buf, chunks = "", []
    for i in range(0, len(parts), 2):
        seg = parts[i] + (parts[i+1] if i+1 < len(parts) else "")
        if len(buf) + len(seg) > max_len:
            if buf:
                chunks.append(buf)
                buf = buf[-overlap:] + seg
            else:
                chunks.append(seg[:max_len])
                buf = seg[max_len-overlap:max_len]
        else:
            buf += seg
    if buf.strip(): chunks.append(buf)
    return [c.strip() for c in chunks if c.strip()]

def batched_embedding(pieces, batch_size=100):
    """åˆ†æ‰¹è®¡ç®— embeddingï¼Œé¿å…è¶… token é™åˆ¶"""
    all_vecs = []
    for i in range(0, len(pieces), batch_size):
        batch = pieces[i:i+batch_size]
        print(f"Embedding batch {i//batch_size+1}/{(len(pieces)-1)//batch_size+1}...")
        try:
            vecs = embed(batch)
            all_vecs.extend(vecs)
            time.sleep(1)  # ç¨å¾®ä¼‘æ¯ï¼Œé˜²æ­¢é€Ÿç‡é™åˆ¶
        except Exception as e:
            print(f"âŒ Batch {i} failed: {e}")
            # å¯ä»¥é‡è¯•æˆ–è·³è¿‡
            all_vecs.extend([[0]*1536]*len(batch))
    return all_vecs

def main():
    ensure_schema()
    raw = read_file_auto_encoding(RAW)
    text = clean_text(raw)
    pieces = chunk_text(text)

    print(f"Total {len(pieces)} chunks to process.")
    con = get_sql(); cur = con.cursor()
    cur.execute("DELETE FROM chunks")
    for i, c in enumerate(pieces):
        cid = f"chunk_{i:05d}"
        cur.execute("INSERT OR REPLACE INTO chunks VALUES (?,?,?,?,?,?)",
                    (cid, "book", "", 0, 0, c))
    con.commit(); con.close()

    # ğŸ”¹ åˆ†æ‰¹è®¡ç®— embedding
    embs = batched_embedding(pieces, batch_size=100)
    add_chunks([f"chunk_{i:05d}" for i in range(len(pieces))], pieces, embs)
    print(f"âœ… Ingested {len(pieces)} chunks and built vector index successfully.")

if __name__ == "__main__":
    if not Path(RAW).exists():
        print(f"âŒ è¯·æŠŠæ•´æœ¬ä¹¦çº¯æ–‡æœ¬æ”¾åˆ° {RAW}")
    else:
        main()
